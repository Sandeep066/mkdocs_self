{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"install/","text":"Prerequisites Kubernetes cluster - An Openshift cluster running on Openshift 4.6+. You will need cluster-admin authority in order to complete all of the prescribed steps. cloudctl - Download latest version of cloudctl CLI from Cloud Pak CLI . oc - Install the latest version of OpenShift CLI from oc CLI . etcd - WML Serving requires an etcd server in order to coordinate internal state which can be either dedicated or shared. More on this later. S3-compatible object storage - Before models can be deployed, a remote S3-compatible datastore is needed from which to pull the model data. This could be for example an IBM Cloud Object Storage instance, or a locally running MinIO deployment. Note that this is not required to be in place prior to the initial installation. Namespace Scope WML Serving is namespace scoped, meaning all of its components must exist within a single namespace and only one instance of WML Serving can be installed per namespace. This applies to the WML Serving Operator as well. Multiple WML Serving instances can be installed in separate namespaces within the cluster. Start your installation by creating a new namespace. oc new-project <namespace-name> Deployed Components Type Pod Count Default CPU request/limit per-pod Default mem request/limit per-pod 1 Operator Common service operator 1 100m / 500m 200Mi / 512Mi 2 Operator ETCD operator 1 100m / 500m 100Mi / 500Mi 3 Operator WML serving operator 1 50m / 500m 40Mi / 40Mi 4 Controller WML serving controller 1 50m / 1 96Mi / 512Mi 5 Metastore ETCD pods 3 200m / 200m 512Mi / 512Mi 6 Built-in Runtime Nvidia Triton runtime Pods 0 (*) 850m / 10 1568Mi / 1984Mi 7 Built-in Runtime The MSP Server runtime Pods 0 (*) 850m / 10 1568Mi / 1984Mi 8 Built-in Runtime The MLServer runtime Pods 0 (*) 850m / 10 1568Mi / 1984Mi totals 7 900m / 3.1 1.925Gi / 3.02Gi (*) ScaleToZero is enabled by default, so runtimes will have 0 replicas until a Predictor is created that uses that runtime. Once a Predictor is assigned, the runtime pods will scale up to 2. When WML serving operator is installed, pods shown in 1 - 3 are created. When WML serving instance is created with default, internal ETCD connection, pods shown in 4 and 5 are created. When WML serving instance is created with external ETCD connection, pod shown in 4 is created. When ScaleToZero is enabled , deployments for runtime pods will be scaled to 0 when there are no Predictors for that runtime. When ScaletoZero is enabled and first predictor CR is submitted, WML serving will spin up the corresponding built-in runtime pods. When ScaletoZero is disabled , pods shown in 6 to 8 are created, with a total CPU(request/limit) of 6/63.1 and total memory(request/limit) of 11.11Gi/14.652Gi. The deployed footprint can be significantly reduced in the following ways: Individual built-in runtimes can be disabled by setting disabled: true in their corresponding ServingRuntime resource - if the corresponding model types aren't used. The number of Pods per runtime can be changed from the default of 2 (e.g. down to 1), via the podsPerRuntime global configuration parameter (see configuration ). It is recommended for this value to be a minimum of 2 for production deployments. Memory and/or CPU resource allocations can be reduced (or increased) on the primary model server container in either of the built-in ServingRuntime resources (container name triton or mleap ). This has the effect of adjusting the total capacity available for holding served models in memory. > oc edit servingruntime triton-2.x > oc edit servingruntime msp-ml-server-0.x > oc edit servingruntime mlserver-0.x Note The MLServer runtime is only present in WML Serving versions >= 0.3.0 Please be aware that: Changes made to the built-in runtime resources will likely be reverted when upgrading/re-installing Most of this resource allocation behaviour/config will change in future versions to become more dynamic - both the number of pods deployed and the system resources allocated to them In addition, the following resources will be created in the same namespace: model-serving-defaults - ConfigMap holding default values tied to a release, should not be modified. Configuration can be overriden by creating a user ConfigMap, see configuration tc-config - ConfigMap used for some internal coordination storage-config - Secret holding config for each of the storage backends from which models can be loaded - see the example Next Steps Install using OLM and operator See the configuration page for details of how to configure system-wide settings via a ConfigMap, either before or after installation. See this example walkthrough of deploying a TensorFlow model as a Predictor .","title":"Getting started"},{"location":"install/#prerequisites","text":"Kubernetes cluster - An Openshift cluster running on Openshift 4.6+. You will need cluster-admin authority in order to complete all of the prescribed steps. cloudctl - Download latest version of cloudctl CLI from Cloud Pak CLI . oc - Install the latest version of OpenShift CLI from oc CLI . etcd - WML Serving requires an etcd server in order to coordinate internal state which can be either dedicated or shared. More on this later. S3-compatible object storage - Before models can be deployed, a remote S3-compatible datastore is needed from which to pull the model data. This could be for example an IBM Cloud Object Storage instance, or a locally running MinIO deployment. Note that this is not required to be in place prior to the initial installation.","title":"Prerequisites"},{"location":"install/#namespace-scope","text":"WML Serving is namespace scoped, meaning all of its components must exist within a single namespace and only one instance of WML Serving can be installed per namespace. This applies to the WML Serving Operator as well. Multiple WML Serving instances can be installed in separate namespaces within the cluster. Start your installation by creating a new namespace. oc new-project <namespace-name>","title":"Namespace Scope"},{"location":"install/#deployed-components","text":"Type Pod Count Default CPU request/limit per-pod Default mem request/limit per-pod 1 Operator Common service operator 1 100m / 500m 200Mi / 512Mi 2 Operator ETCD operator 1 100m / 500m 100Mi / 500Mi 3 Operator WML serving operator 1 50m / 500m 40Mi / 40Mi 4 Controller WML serving controller 1 50m / 1 96Mi / 512Mi 5 Metastore ETCD pods 3 200m / 200m 512Mi / 512Mi 6 Built-in Runtime Nvidia Triton runtime Pods 0 (*) 850m / 10 1568Mi / 1984Mi 7 Built-in Runtime The MSP Server runtime Pods 0 (*) 850m / 10 1568Mi / 1984Mi 8 Built-in Runtime The MLServer runtime Pods 0 (*) 850m / 10 1568Mi / 1984Mi totals 7 900m / 3.1 1.925Gi / 3.02Gi (*) ScaleToZero is enabled by default, so runtimes will have 0 replicas until a Predictor is created that uses that runtime. Once a Predictor is assigned, the runtime pods will scale up to 2. When WML serving operator is installed, pods shown in 1 - 3 are created. When WML serving instance is created with default, internal ETCD connection, pods shown in 4 and 5 are created. When WML serving instance is created with external ETCD connection, pod shown in 4 is created. When ScaleToZero is enabled , deployments for runtime pods will be scaled to 0 when there are no Predictors for that runtime. When ScaletoZero is enabled and first predictor CR is submitted, WML serving will spin up the corresponding built-in runtime pods. When ScaletoZero is disabled , pods shown in 6 to 8 are created, with a total CPU(request/limit) of 6/63.1 and total memory(request/limit) of 11.11Gi/14.652Gi. The deployed footprint can be significantly reduced in the following ways: Individual built-in runtimes can be disabled by setting disabled: true in their corresponding ServingRuntime resource - if the corresponding model types aren't used. The number of Pods per runtime can be changed from the default of 2 (e.g. down to 1), via the podsPerRuntime global configuration parameter (see configuration ). It is recommended for this value to be a minimum of 2 for production deployments. Memory and/or CPU resource allocations can be reduced (or increased) on the primary model server container in either of the built-in ServingRuntime resources (container name triton or mleap ). This has the effect of adjusting the total capacity available for holding served models in memory. > oc edit servingruntime triton-2.x > oc edit servingruntime msp-ml-server-0.x > oc edit servingruntime mlserver-0.x Note The MLServer runtime is only present in WML Serving versions >= 0.3.0 Please be aware that: Changes made to the built-in runtime resources will likely be reverted when upgrading/re-installing Most of this resource allocation behaviour/config will change in future versions to become more dynamic - both the number of pods deployed and the system resources allocated to them In addition, the following resources will be created in the same namespace: model-serving-defaults - ConfigMap holding default values tied to a release, should not be modified. Configuration can be overriden by creating a user ConfigMap, see configuration tc-config - ConfigMap used for some internal coordination storage-config - Secret holding config for each of the storage backends from which models can be loaded - see the example","title":"Deployed Components"},{"location":"install/#next-steps","text":"Install using OLM and operator See the configuration page for details of how to configure system-wide settings via a ConfigMap, either before or after installation. See this example walkthrough of deploying a TensorFlow model as a Predictor .","title":"Next Steps"},{"location":"install/airgap-install/","text":"Step-by-Step Instructions to setup and install wml-serving operator in Airgap environment 0. Setup ENV export CASENAME=ibm-ai-wmlserving export OFFLINEDIR=ibm-ai-wmlserving-archive export NS=airgap-test export ITEM=wmlservingOperatorSetup export ACTION=install-catalog export SOURCE_REGISTRY=cp.stg.icr.io export SOURCE_REGISTRY_USER=xxxxxxxxx export SOURCE_REGISTRY_PASS=xxxxxxxxx export TARGET_REGISTRY=hyc-wml-devops-team-air-gap-test-docker-local.artifactory.swg-devops.com export TARGET_REGISTRY_USER=xxxxxxxxx export TARGET_REGISTRY_PASS=xxxxxxxxx oc new-project $NS 1. Compress the wml-serving case bundle and upload it to airgap cluster git clone git@github.ibm.com:ai-foundation/ibm-ai-wmlserving-case-bundle.git --recursive tar -czvf ibm-ai-wmlserving-case-bundle.tar ibm-ai-wmlserving-case-bundle 2. Extract the latest ibm-ai-wmlserving-case-bundle.tar file tar xzvf ibm-ai-wmlserving-case-bundle.tar Note: To copy case bundle tarball to airgap cluster , Use either one of the approach Use a mount point where the tarball is present Use USBDrive which has the required tarball and copy it to airgap cluster 3. Create a temp docker registry - this act as a customer registry. In this case we use hyc-wml-devops-team-air-gap-test-docker-local.artifactory.swg-devops.com as a customer registry 4. Download the case using cloudctl command cd to case directory cloudctl case save -c ibm-ai-wmlserving -t 1 -o ibm-ai-wmlserving-archive 5. Replace the prod resgitry to the internal registry that contain the paid content image Update ibm-ai-wmlserving-archive/ibm-ai-wmlserving-1.0.0-images.csv file pointing registry to cp.stg.icr.io image_name to cp/ai-foundation/<image_name> 6. Set up temporary registry service (You might need to install this package: sudo yum install httpd-tools -y if it is not installed yet) ibm-ai-wmlserving/inventory/wmlservingOperatorSetup/files/airgap.sh registry service init 7. Create a source registry credential cloudctl case launch \\ --case $CASENAME \\ --namespace $NS \\ --inventory $ITEM \\ --action configure-creds-airgap \\ --args \"--registry $SOURCE_REGISTRY --user $SOURCE_REGISTRY_USER --pass $SOURCE_REGISTRY_PASS\" \\ --tolerance 1 8. Create a destination registry credential cloudctl case launch \\ --case $CASENAME \\ --namespace $NS \\ --inventory $ITEM \\ --action configure-creds-airgap \\ --args \"--registry $TARGET_REGISTRY --user $TARGET_REGISTRY_USER --pass $TARGET_REGISTRY_PASS\" \\ --tolerance 1 9. Mirror the images cloudctl case launch \\ --case $CASENAME \\ --namespace $NS \\ --inventory $ITEM \\ --action mirror-images \\ --args \"--registry $TARGET_REGISTRY --inputDir $OFFLINEDIR\" \\ --tolerance 1 10. Configure cluster with ImageContentSourcePolicy cloudctl case launch \\ --case $CASENAME \\ --namespace $NS \\ --inventory $ITEM \\ --action configure-cluster-airgap \\ --args \"--registry $TARGET_REGISTRY --inputDir $OFFLINEDIR \" \\ --tolerance 1 11. Update the ImageContentSourcePolicy oc edit ImageContentSourcePolicy ibm-ai-wmlserving Add the below entries under spec -- repositoryDigestMirrors - mirrors: - hyc-wml-devops-team-air-gap-test-docker-local.artifactory.swg-devops.com/cp/ai-foundation source: cp.icr.io/cp/ai - mirrors: - hyc-wml-devops-team-air-gap-test-docker-local.artifactory.swg-devops.com/cp/ai-foundation source: icr.io/cpopen Please note that this would restart all the nodes , please wait till all the nodes are restarted and back to Ready state 12. Run the Catalog installation test cloudctl case launch \\ --case $CASENAME \\ --namespace openshift-marketplace \\ --inventory $ITEM \\ --action $ACTION \\ --args \"--registry icr.io --inputDir $OFFLINEDIR --recursive\" \\ --tolerance 1 13. Run the Operator installation test export ACTION=install-operator cloudctl case launch \\ --case $CASENAME \\ --namespace $NS \\ --action $ACTION \\ --inventory $ITEM \\ --tolerance 1","title":"Airgap Installation"},{"location":"install/airgap-install/#step-by-step-instructions-to-setup-and-install-wml-serving-operator-in-airgap-environment","text":"","title":"Step-by-Step Instructions to setup and install wml-serving operator in Airgap environment"},{"location":"install/airgap-install/#0-setup-env","text":"export CASENAME=ibm-ai-wmlserving export OFFLINEDIR=ibm-ai-wmlserving-archive export NS=airgap-test export ITEM=wmlservingOperatorSetup export ACTION=install-catalog export SOURCE_REGISTRY=cp.stg.icr.io export SOURCE_REGISTRY_USER=xxxxxxxxx export SOURCE_REGISTRY_PASS=xxxxxxxxx export TARGET_REGISTRY=hyc-wml-devops-team-air-gap-test-docker-local.artifactory.swg-devops.com export TARGET_REGISTRY_USER=xxxxxxxxx export TARGET_REGISTRY_PASS=xxxxxxxxx oc new-project $NS","title":"0. Setup ENV"},{"location":"install/airgap-install/#1-compress-the-wml-serving-case-bundle-and-upload-it-to-airgap-cluster","text":"git clone git@github.ibm.com:ai-foundation/ibm-ai-wmlserving-case-bundle.git --recursive tar -czvf ibm-ai-wmlserving-case-bundle.tar ibm-ai-wmlserving-case-bundle","title":"1. Compress the wml-serving case bundle and upload it to airgap cluster"},{"location":"install/airgap-install/#2-extract-the-latest-ibm-ai-wmlserving-case-bundletar-file","text":"tar xzvf ibm-ai-wmlserving-case-bundle.tar Note: To copy case bundle tarball to airgap cluster , Use either one of the approach Use a mount point where the tarball is present Use USBDrive which has the required tarball and copy it to airgap cluster","title":"2. Extract the latest ibm-ai-wmlserving-case-bundle.tar file"},{"location":"install/airgap-install/#3-create-a-temp-docker-registry-this-act-as-a-customer-registry-in-this-case-we-use-hyc-wml-devops-team-air-gap-test-docker-localartifactoryswg-devopscom-as-a-customer-registry","text":"","title":"3. Create a temp docker registry - this act as a customer registry. In this case we use hyc-wml-devops-team-air-gap-test-docker-local.artifactory.swg-devops.com as a customer registry"},{"location":"install/airgap-install/#4-download-the-case-using-cloudctl-command","text":"cd to case directory cloudctl case save -c ibm-ai-wmlserving -t 1 -o ibm-ai-wmlserving-archive","title":"4. Download the case using cloudctl command"},{"location":"install/airgap-install/#5-replace-the-prod-resgitry-to-the-internal-registry-that-contain-the-paid-content-image","text":"Update ibm-ai-wmlserving-archive/ibm-ai-wmlserving-1.0.0-images.csv file pointing registry to cp.stg.icr.io image_name to cp/ai-foundation/<image_name>","title":"5. Replace the prod resgitry to the internal registry that contain the paid content image"},{"location":"install/airgap-install/#6-set-up-temporary-registry-service-you-might-need-to-install-this-package-sudo-yum-install-httpd-tools-y-if-it-is-not-installed-yet","text":"ibm-ai-wmlserving/inventory/wmlservingOperatorSetup/files/airgap.sh registry service init","title":"6. Set up temporary registry service (You might need to install this package: sudo yum install httpd-tools -y if it is not installed yet)"},{"location":"install/airgap-install/#7-create-a-source-registry-credential","text":"cloudctl case launch \\ --case $CASENAME \\ --namespace $NS \\ --inventory $ITEM \\ --action configure-creds-airgap \\ --args \"--registry $SOURCE_REGISTRY --user $SOURCE_REGISTRY_USER --pass $SOURCE_REGISTRY_PASS\" \\ --tolerance 1","title":"7. Create a source registry credential"},{"location":"install/airgap-install/#8-create-a-destination-registry-credential","text":"cloudctl case launch \\ --case $CASENAME \\ --namespace $NS \\ --inventory $ITEM \\ --action configure-creds-airgap \\ --args \"--registry $TARGET_REGISTRY --user $TARGET_REGISTRY_USER --pass $TARGET_REGISTRY_PASS\" \\ --tolerance 1","title":"8. Create a destination registry credential"},{"location":"install/airgap-install/#9-mirror-the-images","text":"cloudctl case launch \\ --case $CASENAME \\ --namespace $NS \\ --inventory $ITEM \\ --action mirror-images \\ --args \"--registry $TARGET_REGISTRY --inputDir $OFFLINEDIR\" \\ --tolerance 1","title":"9. Mirror the images"},{"location":"install/airgap-install/#10-configure-cluster-with-imagecontentsourcepolicy","text":"cloudctl case launch \\ --case $CASENAME \\ --namespace $NS \\ --inventory $ITEM \\ --action configure-cluster-airgap \\ --args \"--registry $TARGET_REGISTRY --inputDir $OFFLINEDIR \" \\ --tolerance 1","title":"10. Configure cluster with ImageContentSourcePolicy"},{"location":"install/airgap-install/#11-update-the-imagecontentsourcepolicy","text":"oc edit ImageContentSourcePolicy ibm-ai-wmlserving Add the below entries under spec -- repositoryDigestMirrors - mirrors: - hyc-wml-devops-team-air-gap-test-docker-local.artifactory.swg-devops.com/cp/ai-foundation source: cp.icr.io/cp/ai - mirrors: - hyc-wml-devops-team-air-gap-test-docker-local.artifactory.swg-devops.com/cp/ai-foundation source: icr.io/cpopen Please note that this would restart all the nodes , please wait till all the nodes are restarted and back to Ready state","title":"11. Update the ImageContentSourcePolicy"},{"location":"install/airgap-install/#12-run-the-catalog-installation-test","text":"cloudctl case launch \\ --case $CASENAME \\ --namespace openshift-marketplace \\ --inventory $ITEM \\ --action $ACTION \\ --args \"--registry icr.io --inputDir $OFFLINEDIR --recursive\" \\ --tolerance 1","title":"12. Run the Catalog installation test"},{"location":"install/airgap-install/#13-run-the-operator-installation-test","text":"export ACTION=install-operator cloudctl case launch \\ --case $CASENAME \\ --namespace $NS \\ --action $ACTION \\ --inventory $ITEM \\ --tolerance 1","title":"13. Run the Operator installation test"},{"location":"install/install-script/","text":"Prerequisites Kubernetes cluster - A Kubernetes cluster is required. You will need cluster-admin authority in order to complete all of the prescribed steps. To set up WML Serving for local minikube, review the minikube instructions . Kubectl and Kustomize - The installation will occur via the terminal using kubectl and kustomize . etcd - WML Serving requires an etcd server in order to coordinate internal state which can be either dedicated or shared. More on this later. S3-compatible object storage - Before models can be deployed, a remote S3-compatible datastore is needed from which to pull the model data. This could be for example an IBM Cloud Object Storage instance, or a locally running MinIO deployment. Note that this is not required to be in place prior to the initial installation. We provide an install script to quickly run WML Serving without the operator. This may be useful for experimentation or development but should not be used in production. The install script has a --quickstart option for setting up a self-contained WML Serving instance. This will deploy and configure local etcd and MinIO servers in the same Kubernetes namespace. Note that this is only for experimentation and/or development use - in particular the connections to these datastores are not secure and the etcd cluster is a single member which is not highly available. Use of --quickstart also configures the storage-config secret to be able to pull from the WML Serving example models bucket which contains the model data for the sample Predictors. For complete details on the manfiests applied with --quickstart see config/dependencies/quickstart.yaml . Setup the etcd connection information If the --quickstart install option is not being used, details of an existing etcd cluster must be specified prior to installation. Otherwise, please skip this step and proceed to Installation . Create a file named etcd-config.json, populating the values based upon your etcd server. The same etcd server can be shared between environments and/or namespaces, but in this case the root_prefix must be set differently in each namespace's respective secret . The complete json schema for this configuration is documented here . { \"endpoints\": \"https://etcd-service-hostame:2379\", \"userid\": \"userid\", \"password\": \"password\", \"root_prefix\": \"unique-chroot-prefix\" } Then create the secret using the file (note that the key name within the secret must be etcd_connection ): kubectl create secret generic model-serving-etcd --from-file=etcd_connection=etcd-config.json A secret named model-serving-etcd will be created and passed to the controller. Installation Download and extract the latest install script from Artifactory using the latest release from Github (use your IBM credentials to log in): VERSION=0.4.2_106 DOWNLOAD_URL=https://na.artifactory.swg-devops.com/artifactory/wcp-ai-foundation-team-generic-virtual/model-serving/wml-serving-${VERSION}-install.sh wget --user=you@us.ibm.com --ask-password ${DOWNLOAD_URL} # If you don't have wget you can also use curl curl -u ${artifactory_user}:${artifactory_apikey} -o wml-serving-install.sh ${DOWNLOAD_URL} Run the downloaded script to install WML Serving CRDs, controller, and built-in runtimes into the specified Kubernetes namespaces, after reviewing the command line flags below. An Artifactory user and API key are required to pull the WML Serving tar file and images. These can be passed in as flags or set as environment variables. A Kubernetes --namespace is required, which must already exist. You must also have cluster-admin authority and cluster access must be configured prior to running the install script. An Artifactory user and API key is required if you are pulling the WML Serving tar file and images. These can be passed in as flags or set as environment variables. The --quickstart option can be specified to install and configure supporting datastores in the same namespace (etcd and MinIO) for experimental/development use. If this is not chosen, the namespace provided must have an Etcd secret named model-serving-etcd created which provides access to the Etcd cluster. See the instructions above on this step. $ ./wml-serving-install.sh --help usage: ./wml-serving-install.sh [flags] Flags: -n, --namespace (required) Kubernetes namespace to deploy WML Serving to. -u, --artifactory-user Artifactory username to pull WML serving tarfile and images, can also set with env var ARTIFACTORY_USER. -a, --artifactory-apikey Artifactory API key to pull WML serving tarfile and images, can also set with env var ARTIFACTORY_APIKEY. -v, --model-serve-version WML serving version to pull and use. Example: wml-serving-0.3.0_165 -p, --install-config-path Path to local model serve installation configs. Can be WML Serving tarfile or directory. -d, --delete Delete any existing instances of WML Serving in Kube namespace before running install, including CRDs, RBACs, controller, older CRD with ai.ibm.com api group name, etc. --quickstart Install and configure required supporting datastores in the same namespace (etcd and MinIO) - for experimentation/development --redsonja-images Use images pulled from redsonja IBM Container registry, requires redsonja user and apikey. --redsonja-apikey IBM container registry apikey that has access to redsonja account, can also set with env var REDSONJA_APIKEY. As you can see, you can optionally provide an explicit --model-serve-version for the WML Serving tar file, which will be pulled down from Artifactory via the install script. If not provided, the version installed will be the one corresponding to the version of the install script. Alternatively, you can provide a local --install-config-path that points to a local WML Serving tar file or directory containing WML Serving configs to deploy. You can also optionally use --delete to delete any existing instances of WML Serving in the designated Kube namespace before running the install. The installation will create a secret named storage-config if it does not already exist. If the --quickstart option was chosen, this will be populated with the connection details for the example models bucket in IBM Cloud Object Storage and the local MinIO; otherwise, it will be empty and ready for you to add your own entries. To deploy WML Serving with images from IBM Container Registry (ICR) instead of using the default Artifactory images, add flag --redsonja-images . Note that the Kube dockerconfig secret ibm-entitlement-key in the deployed namespace must have a username (likely iamapikey ) and api key that has access to the redsonja_hyboria/ai-foundation namespace in the redsonja ICR account. If the Kube secret does not already exist, you can provide your --redsonja-apikey <APIKEY> and the secret will be created in the install script. Next Steps Continue with the \"Deployed Components\" section of the Getting Started page.","title":"Non-Operator Installation"},{"location":"install/install-script/#prerequisites","text":"Kubernetes cluster - A Kubernetes cluster is required. You will need cluster-admin authority in order to complete all of the prescribed steps. To set up WML Serving for local minikube, review the minikube instructions . Kubectl and Kustomize - The installation will occur via the terminal using kubectl and kustomize . etcd - WML Serving requires an etcd server in order to coordinate internal state which can be either dedicated or shared. More on this later. S3-compatible object storage - Before models can be deployed, a remote S3-compatible datastore is needed from which to pull the model data. This could be for example an IBM Cloud Object Storage instance, or a locally running MinIO deployment. Note that this is not required to be in place prior to the initial installation. We provide an install script to quickly run WML Serving without the operator. This may be useful for experimentation or development but should not be used in production. The install script has a --quickstart option for setting up a self-contained WML Serving instance. This will deploy and configure local etcd and MinIO servers in the same Kubernetes namespace. Note that this is only for experimentation and/or development use - in particular the connections to these datastores are not secure and the etcd cluster is a single member which is not highly available. Use of --quickstart also configures the storage-config secret to be able to pull from the WML Serving example models bucket which contains the model data for the sample Predictors. For complete details on the manfiests applied with --quickstart see config/dependencies/quickstart.yaml .","title":"Prerequisites"},{"location":"install/install-script/#setup-the-etcd-connection-information","text":"If the --quickstart install option is not being used, details of an existing etcd cluster must be specified prior to installation. Otherwise, please skip this step and proceed to Installation . Create a file named etcd-config.json, populating the values based upon your etcd server. The same etcd server can be shared between environments and/or namespaces, but in this case the root_prefix must be set differently in each namespace's respective secret . The complete json schema for this configuration is documented here . { \"endpoints\": \"https://etcd-service-hostame:2379\", \"userid\": \"userid\", \"password\": \"password\", \"root_prefix\": \"unique-chroot-prefix\" } Then create the secret using the file (note that the key name within the secret must be etcd_connection ): kubectl create secret generic model-serving-etcd --from-file=etcd_connection=etcd-config.json A secret named model-serving-etcd will be created and passed to the controller.","title":"Setup the etcd connection information"},{"location":"install/install-script/#installation","text":"Download and extract the latest install script from Artifactory using the latest release from Github (use your IBM credentials to log in): VERSION=0.4.2_106 DOWNLOAD_URL=https://na.artifactory.swg-devops.com/artifactory/wcp-ai-foundation-team-generic-virtual/model-serving/wml-serving-${VERSION}-install.sh wget --user=you@us.ibm.com --ask-password ${DOWNLOAD_URL} # If you don't have wget you can also use curl curl -u ${artifactory_user}:${artifactory_apikey} -o wml-serving-install.sh ${DOWNLOAD_URL} Run the downloaded script to install WML Serving CRDs, controller, and built-in runtimes into the specified Kubernetes namespaces, after reviewing the command line flags below. An Artifactory user and API key are required to pull the WML Serving tar file and images. These can be passed in as flags or set as environment variables. A Kubernetes --namespace is required, which must already exist. You must also have cluster-admin authority and cluster access must be configured prior to running the install script. An Artifactory user and API key is required if you are pulling the WML Serving tar file and images. These can be passed in as flags or set as environment variables. The --quickstart option can be specified to install and configure supporting datastores in the same namespace (etcd and MinIO) for experimental/development use. If this is not chosen, the namespace provided must have an Etcd secret named model-serving-etcd created which provides access to the Etcd cluster. See the instructions above on this step. $ ./wml-serving-install.sh --help usage: ./wml-serving-install.sh [flags] Flags: -n, --namespace (required) Kubernetes namespace to deploy WML Serving to. -u, --artifactory-user Artifactory username to pull WML serving tarfile and images, can also set with env var ARTIFACTORY_USER. -a, --artifactory-apikey Artifactory API key to pull WML serving tarfile and images, can also set with env var ARTIFACTORY_APIKEY. -v, --model-serve-version WML serving version to pull and use. Example: wml-serving-0.3.0_165 -p, --install-config-path Path to local model serve installation configs. Can be WML Serving tarfile or directory. -d, --delete Delete any existing instances of WML Serving in Kube namespace before running install, including CRDs, RBACs, controller, older CRD with ai.ibm.com api group name, etc. --quickstart Install and configure required supporting datastores in the same namespace (etcd and MinIO) - for experimentation/development --redsonja-images Use images pulled from redsonja IBM Container registry, requires redsonja user and apikey. --redsonja-apikey IBM container registry apikey that has access to redsonja account, can also set with env var REDSONJA_APIKEY. As you can see, you can optionally provide an explicit --model-serve-version for the WML Serving tar file, which will be pulled down from Artifactory via the install script. If not provided, the version installed will be the one corresponding to the version of the install script. Alternatively, you can provide a local --install-config-path that points to a local WML Serving tar file or directory containing WML Serving configs to deploy. You can also optionally use --delete to delete any existing instances of WML Serving in the designated Kube namespace before running the install. The installation will create a secret named storage-config if it does not already exist. If the --quickstart option was chosen, this will be populated with the connection details for the example models bucket in IBM Cloud Object Storage and the local MinIO; otherwise, it will be empty and ready for you to add your own entries. To deploy WML Serving with images from IBM Container Registry (ICR) instead of using the default Artifactory images, add flag --redsonja-images . Note that the Kube dockerconfig secret ibm-entitlement-key in the deployed namespace must have a username (likely iamapikey ) and api key that has access to the redsonja_hyboria/ai-foundation namespace in the redsonja ICR account. If the Kube secret does not already exist, you can provide your --redsonja-apikey <APIKEY> and the secret will be created in the install script.","title":"Installation"},{"location":"install/install-script/#next-steps","text":"Continue with the \"Deployed Components\" section of the Getting Started page.","title":"Next Steps"},{"location":"install/install/","text":"The recommended approach to installing WML Serving is to use OLM and the operator. If you are installing in a development environment and wish to skip the operator, see No Operator Installation . Download the case bundle from https://github.com/IBM/cloud-pak/raw/master/repo/case/ibm-ai-wmlserving-1.0.0.tgz Additional prerequisite steps for non-release version To use the the latest development version of the operator instead of a released one, perform these additional steps before continuing to the \"Set the namespace\" section. 1. Clone the case bundle repo git clone git@github.ibm.com:ai-foundation/ibm-ai-wmlserving-case-bundle.git 2. Setup a registry mirror Create an image mirror on the cluster which points to the staging registry. apiVersion: operator.openshift.io/v1alpha1 kind: ImageContentSourcePolicy metadata: name: mirror-config spec: repositoryDigestMirrors: - mirrors: - cp.stg.icr.io/cp/ai source: cp.icr.io/cp/ai - mirrors: - cp.stg.icr.io/cp source: icr.io/cpopen This is will restart the nodes of the cluster one by one. Wait for all the nodes to be restarted. 3. Setup a global pull secret a) Take a backup of the existing global pull secret. oc get secret/pull-secret -n openshift-config -o yaml > pull-secret-bk.yaml b) Update it to include credentials for cp.stg.icr.io . pull_secret=$(echo -n \"iamapikey:<APIKEY>\" | base64 -w0) oc get secret/pull-secret -n openshift-config -o jsonpath='{.data.\\.dockerconfigjson}' | base64 -d | sed -e 's|\"auths\":{|\"auths\":{\"cp.stg.icr.io\":{\"auth\":\"'$pull_secret'\"\\},|' > /tmp/dockerconfig.json oc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=/tmp/dockerconfig.json Follow the below links to get your own api key: https://playbook.cloudpaklab.ibm.com/entitled-registry-access/ https://playbook.cloudpaklab.ibm.com/entitled-image-registry/#Read_Only_Access Install WML Serving operator 1. Set the namespace export NAMESPACE=<project-name> 2. Configure image pull secret You must create a pull secret called ibm-entitlement-key in the target namespace. For more information see the CloudPak Playbook oc create secret docker-registry ibm-entitlement-key -n $NAMESPACE \\ --docker-server=cp.icr.io \\ --docker-username=\"cp\" \\ --docker-password=\"<APIKEY>\" Get the apikey from here: https://myibm.ibm.com/products-services/containerlibrary 3. Setup etcd etcd is required to run WML Serving. You can either bring your own etcd or install the ibm-etcd-operator-catalog so that the wmlserving-operator can create etcd for you. Bring your own etcd Create a file named etcd-config.json , populating the values based upon your etcd server. The same etcd server can be shared between environments and/or namespaces, but in this case the root_prefix must be set differently in each namespace's respective secret . The complete json schema for this configuration is documented here . { \"endpoints\": \"https://etcd-service-hostame:2379\", \"userid\": \"userid\", \"password\": \"password\", \"root_prefix\": \"unique-chroot-prefix\" } Then create the secret using the file (note that the key name within the secret must be etcd_connection ): oc create secret generic model-serving-etcd --from-file=etcd_connection=etcd-config.json Place this secret name in the WmlServing CR which you will create in a later section. apiVersion: ai.ibm.com/v1 kind: WmlServing metadata: name: wmlserving-sample namespace: $NAMESPACE spec: etcd: externalConnection: secretName: model-serving-etcd 4. Install the ibm-etcd-operator-catalog Deploy etcd catalog from https://github.ibm.com/CloudPakOpenContent/ibm-etcd-operator/ apiVersion: operators.coreos.com/v1alpha1 kind: CatalogSource metadata: name: ibm-etcd-operator-catalog namespace: openshift-marketplace spec: displayName: IBM etcd operator Catalog publisher: IBM sourceType: grpc image: icr.io/cpopen/ibm-etcd-operator-catalog@sha256:dd1687d02b8fb35bc08464330471d6f6f6541c36ca56d2716d02fa071a5cc754 updateStrategy: registryPoll: interval: 45m Note: The etcd-operator requires dynamic storage provisioning in order to create persistent volume claims. For Fyre clusters, you can ssh to the inf node and run this script to setup an nfs provisioner. Other dynamic storage providers are available but out of scope of this document. 5. Install the opencloud-operators catalog Deploy the IBM Cloud Pak foundational services catalog source which is used to install cert-manager. apiVersion: operators.coreos.com/v1alpha1 kind: CatalogSource metadata: name: opencloud-operators namespace: openshift-marketplace spec: displayName: IBMCS Operators publisher: IBM sourceType: grpc image: icr.io/cpopen/ibm-common-service-catalog:latest updateStrategy: registryPoll: interval: 45m 6. Deploy the WML Serving Operator catalog cloudctl case launch --case ibm-ai-wmlserving-1.0.0.tgz --namespace openshift-marketplace --inventory wmlservingOperatorSetup --action install-catalog --args \"--registry icr.io --inputDir /tmp/saved --recursive\" --tolerance 1 7. Install the WML Serving Operator cloudctl case launch --case ibm-ai-wmlserving-1.0.0.tgz --tolerance 1 -n $NAMESPACE --action install-operator --inventory wmlservingOperatorSetup All three operator pods (wmlserving, etcd and ibm-common-service) should be up and running. $ oc get pods NAME READY STATUS RESTARTS AGE ibm-common-service-operator-8669686f89-64kcn 1/1 Running 0 5m20s ibm-etcd-operator-65d44f5fb7-5fkwq 1/1 Running 0 5m22s wmlserving-operator-74ffcb9976-hf7lf 1/1 Running 0 5m18s 8. Create a WmlServing Instance To create an instance of WML Serving and start the controller, you must create a WmlServing resource. The following sample uses the default config: Note: It is mandatory to provide the storageClass of your dynamic storage provisioner for etcd. For details of the supported storage classes refer to the etcd operator documentation . Currently, WML Serving CR doesn't support Update/Patch. If any change should be applied on the CR, it should be deleted and recreated again. oc apply -f - <<EOF apiVersion: ai.ibm.com/v1 kind: WmlServing metadata: name: wmlserving-sample namespace: $NAMESPACE spec: etcd: storageClass: \"managed-nfs-storage\" EOF 9. Check WmlServing CR status oc get wmlserving wmlserving-sample NAME READY wmlserving-sample True When the installation is complete, you should see the following pods: $ oc get pods NAME READY STATUS RESTARTS AGE ibm-common-service-operator-8669686f89-64kcn 1/1 Running 0 5m20s ibm-etcd-operator-65d44f5fb7-5fkwq 1/1 Running 0 5m22s wmlserving-operator-74ffcb9976-hf7lf 1/1 Running 0 5m18s wmlserving-controller-7dd947d49c-7x4sr 1/1 Running 0 3m18s wmlserving-sample-etcd-0 1/1 Running 0 3m9s wmlserving-sample-etcd-1 1/1 Running 0 2m58s wmlserving-sample-etcd-2 1/1 Running 0 2m39s By default, built-in runtime pods will not be up as the ScaleToZero configuration parameter is enabled. The corresponding runtime pod spins up when ever a Predictor CR gets submitted. Alternatively, ScaleToZero configuration parameter can also be disabled (see configuration ) so that WML Serving will spin up all the built-in runtime pods as shown below: wml-serving-mlserver-0.x-8499d6f846-kccf2 3/3 Running 0 7m59s wml-serving-mlserver-0.x-66cd794bd5-hvpkp 3/3 Running 0 7m59s wml-serving-msp-ml-server-0.x-67ddb449bb-tmhnc 3/3 Running 0 7m58s wml-serving-msp-ml-server-0.x-67ddb449bb-xx27p 3/3 Running 0 7m58s wml-serving-triton-2.x-7cf854dbf4-ft9qd 3/3 Running 0 7m58s wml-serving-triton-2.x-7cf854dbf4-mkmxc 3/3 Running 0 7m58s Note: The video demonstration on how to install WML Serving operator is available here .","title":"Operator-based Installation"},{"location":"install/install/#additional-prerequisite-steps-for-non-release-version","text":"To use the the latest development version of the operator instead of a released one, perform these additional steps before continuing to the \"Set the namespace\" section.","title":"Additional prerequisite steps for non-release version"},{"location":"install/install/#1-clone-the-case-bundle-repo","text":"git clone git@github.ibm.com:ai-foundation/ibm-ai-wmlserving-case-bundle.git","title":"1. Clone the case bundle repo"},{"location":"install/install/#2-setup-a-registry-mirror","text":"Create an image mirror on the cluster which points to the staging registry. apiVersion: operator.openshift.io/v1alpha1 kind: ImageContentSourcePolicy metadata: name: mirror-config spec: repositoryDigestMirrors: - mirrors: - cp.stg.icr.io/cp/ai source: cp.icr.io/cp/ai - mirrors: - cp.stg.icr.io/cp source: icr.io/cpopen This is will restart the nodes of the cluster one by one. Wait for all the nodes to be restarted.","title":"2. Setup a registry mirror"},{"location":"install/install/#3-setup-a-global-pull-secret","text":"","title":"3. Setup a global pull secret"},{"location":"install/install/#a-take-a-backup-of-the-existing-global-pull-secret","text":"oc get secret/pull-secret -n openshift-config -o yaml > pull-secret-bk.yaml","title":"a) Take a backup of the existing global pull secret."},{"location":"install/install/#b-update-it-to-include-credentials-for-cpstgicrio","text":"pull_secret=$(echo -n \"iamapikey:<APIKEY>\" | base64 -w0) oc get secret/pull-secret -n openshift-config -o jsonpath='{.data.\\.dockerconfigjson}' | base64 -d | sed -e 's|\"auths\":{|\"auths\":{\"cp.stg.icr.io\":{\"auth\":\"'$pull_secret'\"\\},|' > /tmp/dockerconfig.json oc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=/tmp/dockerconfig.json Follow the below links to get your own api key: https://playbook.cloudpaklab.ibm.com/entitled-registry-access/ https://playbook.cloudpaklab.ibm.com/entitled-image-registry/#Read_Only_Access","title":"b) Update it to include credentials for cp.stg.icr.io."},{"location":"install/install/#install-wml-serving-operator","text":"","title":"Install WML Serving operator"},{"location":"install/install/#1-set-the-namespace","text":"export NAMESPACE=<project-name>","title":"1. Set the namespace"},{"location":"install/install/#2-configure-image-pull-secret","text":"You must create a pull secret called ibm-entitlement-key in the target namespace. For more information see the CloudPak Playbook oc create secret docker-registry ibm-entitlement-key -n $NAMESPACE \\ --docker-server=cp.icr.io \\ --docker-username=\"cp\" \\ --docker-password=\"<APIKEY>\" Get the apikey from here: https://myibm.ibm.com/products-services/containerlibrary","title":"2. Configure image pull secret"},{"location":"install/install/#3-setup-etcd","text":"etcd is required to run WML Serving. You can either bring your own etcd or install the ibm-etcd-operator-catalog so that the wmlserving-operator can create etcd for you.","title":"3. Setup etcd"},{"location":"install/install/#bring-your-own-etcd","text":"Create a file named etcd-config.json , populating the values based upon your etcd server. The same etcd server can be shared between environments and/or namespaces, but in this case the root_prefix must be set differently in each namespace's respective secret . The complete json schema for this configuration is documented here . { \"endpoints\": \"https://etcd-service-hostame:2379\", \"userid\": \"userid\", \"password\": \"password\", \"root_prefix\": \"unique-chroot-prefix\" } Then create the secret using the file (note that the key name within the secret must be etcd_connection ): oc create secret generic model-serving-etcd --from-file=etcd_connection=etcd-config.json Place this secret name in the WmlServing CR which you will create in a later section. apiVersion: ai.ibm.com/v1 kind: WmlServing metadata: name: wmlserving-sample namespace: $NAMESPACE spec: etcd: externalConnection: secretName: model-serving-etcd","title":"Bring your own etcd"},{"location":"install/install/#4-install-the-ibm-etcd-operator-catalog","text":"Deploy etcd catalog from https://github.ibm.com/CloudPakOpenContent/ibm-etcd-operator/ apiVersion: operators.coreos.com/v1alpha1 kind: CatalogSource metadata: name: ibm-etcd-operator-catalog namespace: openshift-marketplace spec: displayName: IBM etcd operator Catalog publisher: IBM sourceType: grpc image: icr.io/cpopen/ibm-etcd-operator-catalog@sha256:dd1687d02b8fb35bc08464330471d6f6f6541c36ca56d2716d02fa071a5cc754 updateStrategy: registryPoll: interval: 45m Note: The etcd-operator requires dynamic storage provisioning in order to create persistent volume claims. For Fyre clusters, you can ssh to the inf node and run this script to setup an nfs provisioner. Other dynamic storage providers are available but out of scope of this document.","title":"4. Install the ibm-etcd-operator-catalog"},{"location":"install/install/#5-install-the-opencloud-operators-catalog","text":"Deploy the IBM Cloud Pak foundational services catalog source which is used to install cert-manager. apiVersion: operators.coreos.com/v1alpha1 kind: CatalogSource metadata: name: opencloud-operators namespace: openshift-marketplace spec: displayName: IBMCS Operators publisher: IBM sourceType: grpc image: icr.io/cpopen/ibm-common-service-catalog:latest updateStrategy: registryPoll: interval: 45m","title":"5. Install the opencloud-operators catalog"},{"location":"install/install/#6-deploy-the-wml-serving-operator-catalog","text":"cloudctl case launch --case ibm-ai-wmlserving-1.0.0.tgz --namespace openshift-marketplace --inventory wmlservingOperatorSetup --action install-catalog --args \"--registry icr.io --inputDir /tmp/saved --recursive\" --tolerance 1","title":"6. Deploy the WML Serving Operator catalog"},{"location":"install/install/#7-install-the-wml-serving-operator","text":"cloudctl case launch --case ibm-ai-wmlserving-1.0.0.tgz --tolerance 1 -n $NAMESPACE --action install-operator --inventory wmlservingOperatorSetup All three operator pods (wmlserving, etcd and ibm-common-service) should be up and running. $ oc get pods NAME READY STATUS RESTARTS AGE ibm-common-service-operator-8669686f89-64kcn 1/1 Running 0 5m20s ibm-etcd-operator-65d44f5fb7-5fkwq 1/1 Running 0 5m22s wmlserving-operator-74ffcb9976-hf7lf 1/1 Running 0 5m18s","title":"7. Install the WML Serving Operator"},{"location":"install/install/#8-create-a-wmlserving-instance","text":"To create an instance of WML Serving and start the controller, you must create a WmlServing resource. The following sample uses the default config: Note: It is mandatory to provide the storageClass of your dynamic storage provisioner for etcd. For details of the supported storage classes refer to the etcd operator documentation . Currently, WML Serving CR doesn't support Update/Patch. If any change should be applied on the CR, it should be deleted and recreated again. oc apply -f - <<EOF apiVersion: ai.ibm.com/v1 kind: WmlServing metadata: name: wmlserving-sample namespace: $NAMESPACE spec: etcd: storageClass: \"managed-nfs-storage\" EOF","title":"8. Create a WmlServing Instance"},{"location":"install/install/#9-check-wmlserving-cr-status","text":"oc get wmlserving wmlserving-sample NAME READY wmlserving-sample True When the installation is complete, you should see the following pods: $ oc get pods NAME READY STATUS RESTARTS AGE ibm-common-service-operator-8669686f89-64kcn 1/1 Running 0 5m20s ibm-etcd-operator-65d44f5fb7-5fkwq 1/1 Running 0 5m22s wmlserving-operator-74ffcb9976-hf7lf 1/1 Running 0 5m18s wmlserving-controller-7dd947d49c-7x4sr 1/1 Running 0 3m18s wmlserving-sample-etcd-0 1/1 Running 0 3m9s wmlserving-sample-etcd-1 1/1 Running 0 2m58s wmlserving-sample-etcd-2 1/1 Running 0 2m39s By default, built-in runtime pods will not be up as the ScaleToZero configuration parameter is enabled. The corresponding runtime pod spins up when ever a Predictor CR gets submitted. Alternatively, ScaleToZero configuration parameter can also be disabled (see configuration ) so that WML Serving will spin up all the built-in runtime pods as shown below: wml-serving-mlserver-0.x-8499d6f846-kccf2 3/3 Running 0 7m59s wml-serving-mlserver-0.x-66cd794bd5-hvpkp 3/3 Running 0 7m59s wml-serving-msp-ml-server-0.x-67ddb449bb-tmhnc 3/3 Running 0 7m58s wml-serving-msp-ml-server-0.x-67ddb449bb-xx27p 3/3 Running 0 7m58s wml-serving-triton-2.x-7cf854dbf4-ft9qd 3/3 Running 0 7m58s wml-serving-triton-2.x-7cf854dbf4-mkmxc 3/3 Running 0 7m58s Note: The video demonstration on how to install WML Serving operator is available here .","title":"9. Check WmlServing CR status"},{"location":"install/minikube/","text":"Minikube This guide provides instructions for using WML Serving with minikube as well as some common development workflows. Start minikube minikube start --vm-driver=virtualbox Setup Namespaces kubectl create namespace wml-serving kubectl config set-context --current --namespace=wml-serving Start and configure Etcd Startup a new terminal session where the etcd server will be hosted. Run the command: etcd --listen-client-urls http://0.0.0.0:2379 --advertise-client-urls http://0.0.0.0:2379 Etcd will be listening on your local host address, but since minikube will be running in a virtual machine you will need to know the host address to setup the communication. On mac systems you can find your local address with the command: ifconfig | grep -A 1 'en0' | tail -1 | cut -d ':' -f 2 | cut -d ' ' -f 2 Next, create a file named etcd-config.json with the contents, making sure to enter your local address: { \"endpoints\": \"http://<host ip>:2379\", \"root_prefix\": \"model-mesh\" } Use that file to create a secret: kubectl create secret generic model-serving-etcd --from-file=etcd_connection=etcd-config.json Override Configuration Defaults To override configuration defaults, create a config map in the current namespace: apiVersion: v1 kind: ConfigMap metadata: name: model-serving-config data: config.yaml: | # Sample config overrides #inferenceServiceName: \"model-mesh\" #inferenceServicePort: 8033 #podsPerDeployment: 2 #modelMesh: # image: us.icr.io/model-mesh/model-mesh # tag: main-20201029-3 #puller: # image: us.icr.io/model-mesh/model-serving-puller # tag: develop-20201103-3 Workflows Building the image and running the deployment #install the CRDs, just do this once on this cluster unless you are modifying CRDs make install # set the docker env to point to minikube eval $(minikube -p minikube docker-env) # build the image into the minikube docker env make build # deploy make deploy # If you are on minikube without a registry, you need to set the image pull policy to not pull kubectl patch deployment wmlserving-controller --type=json -p='[{\"op\": \"replace\", \"path\": \"/spec/template/spec/containers/0/imagePullPolicy\", \"value\": \"Never\"}]' Building an image with a registry Set the IMG environment variable to build the image with the target registry name: IMG=myregistry/myimage:tag make build Push it docker push myregistry/myimage:tag Then deploy it: IMG=myregistry/myimage:tag make deploy Run the controller in terminal Note that this will run the controller in your current kube context so the behavior will differ slightly from the deployment which runs on cluster. # Scale down the deployment to 0 kubectl scale --replicas=0 deployment/wmlserving-controller # Run the controller ETCD_SECRET_NAME=model-mesh-etcd go run *.go Modifying CRDs Regenerate the CRDs: make manifests generate Assure they install: make install Pre-integration verification # Run formatter, linter, and tests, build the image # prefixing task list with 'run' task will run these in a docker container make run build test","title":"Minikube Installation"},{"location":"install/minikube/#minikube","text":"This guide provides instructions for using WML Serving with minikube as well as some common development workflows.","title":"Minikube"},{"location":"install/minikube/#start-minikube","text":"minikube start --vm-driver=virtualbox","title":"Start minikube"},{"location":"install/minikube/#setup-namespaces","text":"kubectl create namespace wml-serving kubectl config set-context --current --namespace=wml-serving","title":"Setup Namespaces"},{"location":"install/minikube/#start-and-configure-etcd","text":"Startup a new terminal session where the etcd server will be hosted. Run the command: etcd --listen-client-urls http://0.0.0.0:2379 --advertise-client-urls http://0.0.0.0:2379 Etcd will be listening on your local host address, but since minikube will be running in a virtual machine you will need to know the host address to setup the communication. On mac systems you can find your local address with the command: ifconfig | grep -A 1 'en0' | tail -1 | cut -d ':' -f 2 | cut -d ' ' -f 2 Next, create a file named etcd-config.json with the contents, making sure to enter your local address: { \"endpoints\": \"http://<host ip>:2379\", \"root_prefix\": \"model-mesh\" } Use that file to create a secret: kubectl create secret generic model-serving-etcd --from-file=etcd_connection=etcd-config.json","title":"Start and configure Etcd"},{"location":"install/minikube/#override-configuration-defaults","text":"To override configuration defaults, create a config map in the current namespace: apiVersion: v1 kind: ConfigMap metadata: name: model-serving-config data: config.yaml: | # Sample config overrides #inferenceServiceName: \"model-mesh\" #inferenceServicePort: 8033 #podsPerDeployment: 2 #modelMesh: # image: us.icr.io/model-mesh/model-mesh # tag: main-20201029-3 #puller: # image: us.icr.io/model-mesh/model-serving-puller # tag: develop-20201103-3","title":"Override Configuration Defaults"},{"location":"install/minikube/#workflows","text":"","title":"Workflows"},{"location":"install/minikube/#building-the-image-and-running-the-deployment","text":"#install the CRDs, just do this once on this cluster unless you are modifying CRDs make install # set the docker env to point to minikube eval $(minikube -p minikube docker-env) # build the image into the minikube docker env make build # deploy make deploy # If you are on minikube without a registry, you need to set the image pull policy to not pull kubectl patch deployment wmlserving-controller --type=json -p='[{\"op\": \"replace\", \"path\": \"/spec/template/spec/containers/0/imagePullPolicy\", \"value\": \"Never\"}]'","title":"Building the image and running the deployment"},{"location":"install/minikube/#building-an-image-with-a-registry","text":"Set the IMG environment variable to build the image with the target registry name: IMG=myregistry/myimage:tag make build Push it docker push myregistry/myimage:tag Then deploy it: IMG=myregistry/myimage:tag make deploy","title":"Building an image with a registry"},{"location":"install/minikube/#run-the-controller-in-terminal","text":"Note that this will run the controller in your current kube context so the behavior will differ slightly from the deployment which runs on cluster. # Scale down the deployment to 0 kubectl scale --replicas=0 deployment/wmlserving-controller # Run the controller ETCD_SECRET_NAME=model-mesh-etcd go run *.go","title":"Run the controller in terminal"},{"location":"install/minikube/#modifying-crds","text":"Regenerate the CRDs: make manifests generate Assure they install: make install","title":"Modifying CRDs"},{"location":"install/minikube/#pre-integration-verification","text":"# Run formatter, linter, and tests, build the image # prefixing task list with 'run' task will run these in a docker container make run build test","title":"Pre-integration verification"},{"location":"install/uninstall/","text":"Uninstallation of WMLServing Operator/Catalog Cleanup existing Predictors Execute the below command to delete all predictors at once: oc delete predictor --all -n $NAMESPACE Delete WMLServing instance oc delete wmlserving <wmlserving_CR_name> -n $NAMESPACE Note : The above command deletes the underlying etcdCluster instance, but the corresponding etcd PVCs are not deleted to avoid data loss. It is recommended to take a backup if required and delete them to avoid reusing with new WMLServing instance of same name. The existing metadata in pvc might create conflict with new data that will be generated. Following naming convention is used for etcd PVCs: data-<wmlserving_CR_name>-etcd-<replica no> For more details, refer to etcd documentation Uninstall WMLServing Operator cloudctl case launch \\ --case ibm-ai-wmlserving-1.0.0.tgz \\ --tolerance 1 \\ --namespace $NAMESPACE \\ --action uninstall-operator \\ --inventory wmlservingOperatorSetup Uninstall WMLServing Catalog cloudctl case launch \\ --case ibm-ai-wmlserving-1.0.0.tgz \\ --namespace openshift-marketplace \\ --inventory wmlservingOperatorSetup \\ --action uninstall-catalog \\ --tolerance 1 Uninstall the ibm-common-service/ibm-etcd operators Delete the operator subscription: Run the following command to check the subscriptions: oc get subscription -n $NAMESPACE Delete the subscriptions for both ibm-common-service-operator and ibm-etcd-operator using the below command: oc delete subscription <subscription_name> -n $NAMESPACE Delete Cluster Service Version (CSV): Run the following command to check the CSVs: oc get csv -n $NAMESPACE Delete the CSVs for both ibm-common-service-operator and ibm-etcd-operator using the below command: oc delete csv <csv_name> -n $NAMESPACE Delete Custom Resource Definitions oc delete CustomResourceDefinition wmlservings.ai.ibm.com oc delete CustomResourceDefinition servingruntimes.wmlserving.ai.ibm.com oc delete CustomResourceDefinition predictors.wmlserving.ai.ibm.com","title":"Uninstalling WML Serving Operator/Catalog"},{"location":"install/uninstall/#uninstallation-of-wmlserving-operatorcatalog","text":"Cleanup existing Predictors Execute the below command to delete all predictors at once: oc delete predictor --all -n $NAMESPACE Delete WMLServing instance oc delete wmlserving <wmlserving_CR_name> -n $NAMESPACE Note : The above command deletes the underlying etcdCluster instance, but the corresponding etcd PVCs are not deleted to avoid data loss. It is recommended to take a backup if required and delete them to avoid reusing with new WMLServing instance of same name. The existing metadata in pvc might create conflict with new data that will be generated. Following naming convention is used for etcd PVCs: data-<wmlserving_CR_name>-etcd-<replica no> For more details, refer to etcd documentation Uninstall WMLServing Operator cloudctl case launch \\ --case ibm-ai-wmlserving-1.0.0.tgz \\ --tolerance 1 \\ --namespace $NAMESPACE \\ --action uninstall-operator \\ --inventory wmlservingOperatorSetup Uninstall WMLServing Catalog cloudctl case launch \\ --case ibm-ai-wmlserving-1.0.0.tgz \\ --namespace openshift-marketplace \\ --inventory wmlservingOperatorSetup \\ --action uninstall-catalog \\ --tolerance 1 Uninstall the ibm-common-service/ibm-etcd operators Delete the operator subscription: Run the following command to check the subscriptions: oc get subscription -n $NAMESPACE Delete the subscriptions for both ibm-common-service-operator and ibm-etcd-operator using the below command: oc delete subscription <subscription_name> -n $NAMESPACE Delete Cluster Service Version (CSV): Run the following command to check the CSVs: oc get csv -n $NAMESPACE Delete the CSVs for both ibm-common-service-operator and ibm-etcd-operator using the below command: oc delete csv <csv_name> -n $NAMESPACE Delete Custom Resource Definitions oc delete CustomResourceDefinition wmlservings.ai.ibm.com oc delete CustomResourceDefinition servingruntimes.wmlserving.ai.ibm.com oc delete CustomResourceDefinition predictors.wmlserving.ai.ibm.com","title":"Uninstallation of WMLServing Operator/Catalog"},{"location":"predictors/","text":"Trained models are deployed in WML Serving via Predictor s. These represent a stable service endpoint behind which the underlying model can change. Models must reside on shared storage. Currently, only S3-based storage is supported but support for other types will follow. Note that model data residing at a particular path within a given storage instance is assumed to be immutable . Different versions of the same logical model are treated at the base level as independent models and must reside at different paths. In particular, where a given model server/runtime natively supports the notion of versioning (such as Nvidia Triton, TensorFlow Serving, etc), the provided path should not point to the top of a (pseudo-)directory structure containing multiple versions. Instead, point to the subdirectory which corresponds to a specific version. Deploying a scikit-learn model Prerequisites The WML Serving instance should be installed in the desired namespace. See install docs for more details. Deploy a sample model directly from our shared object storage Check the storage-config secret for access to shared COS instance A set of example models are shared via an IBM Cloud COS instance to use when getting started with WML Serving and experimenting with the provided runtimes. Access to this COS instance is set up in the storage-config secret. If you used quick start install then there will be a key within the storage-config secret already configured with the name wml-serving-example-models . If you installed WML Serving using the operator, you will have to configure the storage-config secret for access: $ kubectl patch secret/storage-config -p '{\"data\": {\"wml-serving-example-models\": \"ewogICJ0eXBlIjogInMzIiwKICAiYWNjZXNzX2tleV9pZCI6ICJlY2I5ODNmMTE4MjI0MjNjYTllNDg3Zjg5OGQ1NGE4ZiIsCiAgInNlY3JldF9hY2Nlc3Nfa2V5IjogImNkYmVmZjZhMzJhZWY2YzIzNzRhZTY5ZWVmNTAzZTZkZDBjOTNkNmE3NGJjMjQ2NyIsCiAgImVuZHBvaW50X3VybCI6ICJodHRwczovL3MzLnVzLXNvdXRoLmNsb3VkLW9iamVjdC1zdG9yYWdlLmFwcGRvbWFpbi5jbG91ZCIsCiAgInJlZ2lvbiI6ICJ1cy1zb3V0aCIsCiAgImRlZmF1bHRfYnVja2V0IjogIndtbC1zZXJ2aW5nLWV4YW1wbGUtbW9kZWxzLXB1YmxpYyIKfQo=\"}}' For reference the contents of the secret value for the wml-serving-example-models entry looks like: { \"type\": \"s3\", \"access_key_id\": \"ecb983f11822423ca9e487f898d54a8f\", \"secret_access_key\": \"cdbeff6a32aef6c2374ae69eef503e6dd0c93d6a74bc2467\", \"endpoint_url\": \"https://s3.us-south.cloud-object-storage.appdomain.cloud\", \"region\": \"us-south\", \"default_bucket\": \"wml-serving-example-models-public\" } Note After updating the storage config secret, there may be a delay of up to 2 minutes until the change is picked up. You should take this into account when creating/updating Predictors that use storage keys which have just been added or updated - they may fail to load otherwise. For more details of configuring model storage, see the Setup Storage page. Create a Predictor Custom Resource to serve the sample model The config/example-predictors directory contains Predictor manifests for many of the example models. For a list of available models, see the example models documentation . Here we are deploying an sklearn model located at sklearn/mnist-svm.joblib within the shared COS storage. # Pulled from sample config/example-predictors/example-mlserver-sklearn-mnist-predictor.yaml $ kubectl apply -f - <<EOF apiVersion: wmlserving.ai.ibm.com/v1 kind: Predictor metadata: name: example-mnist-predictor spec: modelType: name: sklearn path: sklearn/mnist-svm.joblib storage: s3: secretKey: wml-serving-example-models EOF predictor.wmlserving.ai.ibm.com/example-mnist-predictor created Note that wml-serving-example-models is the name of the secret key created/verified in the previous step. For more details go to the Predictor Spec page . Once the Predictor is created, mlserver runtime pods are automatically started to load and serve it. $ kubectl get pods NAME READY STATUS RESTARTS AGE wml-serving-mlserver-0.x-658b7dd689-46nwm 0/3 ContainerCreating 0 2s wml-serving-mlserver-0.x-658b7dd689-46nwm 0/3 ContainerCreating 0 2s wmlserving-controller-568c45b959-nl88c 1/1 Running 0 11m Check the status of your Predictor: $ kubectl get predictors NAME TYPE AVAILABLE ACTIVEMODEL TARGETMODEL TRANSITION AGE example-mnist-predictor sklearn true Loading UpToDate 60s $ kubectl get predictor example-mnist-predictor -o=jsonpath='{.status.grpcEndpoint}' grpc://wml-serving:8033 The states should reflect immediate availability, but may take some seconds to move from Loading to Loaded . Inferencing requests for this Predictor received prior to loading completion will block until it completes. Note: When ScaleToZero is enabled, the first Predictor assigned to the Triton runtime may be stuck in the Pending state for some time while the Triton pods are being created. The Triton image is large and may take a while to download. Using the deployed model Configure your gRPC client to point to address wml-serving:8033 . Use the protobuf-based gRPC inference service defined here to make inference requests to the model using the ModelInfer RPC, setting the name of the Predictor as the model_name field in the ModelInferRequest message. Here is an example of how to do this using the command-line based grpcurl : Port-forward to access the runtime service: # access via localhost:8033 $ kubectl port-forward service/wml-serving 8033 Forwarding from 127.0.0.1:8033 -> 8033 Forwarding from [::1]:8033 -> 8033 In a separate terminal window, send an inference request using the proto file from fvt/proto or one that you have locally. Note that you have to provide the model_name in the data load, which is the name of the Predictor deployed. $ grpcurl -plaintext -proto fvt/proto/kfs_inference_v2.proto localhost:8033 list inference.GRPCInferenceService # run inference # with below input, expect output to be 8 $ grpcurl -plaintext -proto fvt/proto/kfs_inference_v2.proto -d '{ \"model_name\": \"example-mnist-predictor\", \"inputs\": [{ \"name\": \"predict\", \"shape\": [1, 64], \"datatype\": \"FP32\", \"contents\": { \"fp32_contents\": [0.0, 0.0, 1.0, 11.0, 14.0, 15.0, 3.0, 0.0, 0.0, 1.0, 13.0, 16.0, 12.0, 16.0, 8.0, 0.0, 0.0, 8.0, 16.0, 4.0, 6.0, 16.0, 5.0, 0.0, 0.0, 5.0, 15.0, 11.0, 13.0, 14.0, 0.0, 0.0, 0.0, 0.0, 2.0, 12.0, 16.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 13.0, 16.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 16.0, 16.0, 16.0, 7.0, 0.0, 0.0, 0.0, 0.0, 11.0, 13.0, 12.0, 1.0, 0.0] }}]}' localhost:8033 inference.GRPCInferenceService.ModelInfer { \"modelName\": \"example-mnist-predictor-725d74f061\", \"outputs\": [ { \"name\": \"predict\", \"datatype\": \"FP32\", \"shape\": [ \"1\" ], \"contents\": { \"fp32Contents\": [ 8 ] } } ] } Updating the model Changes can be made to the Predictor's Spec, such as changing the target storage and/or model, without interrupting the inferencing service. The predictor will continue to use the prior spec/model until the new one is loaded and ready. Below, we are changing the Predictor to use a completely different model, in practice the schema of the Predictor's model would be consistent across updates even if the type of model or ML framework changes. $ kubectl apply -f - <<EOF apiVersion: wmlserving.ai.ibm.com/v1 kind: Predictor metadata: name: example-mnist-predictor spec: modelType: name: tensorflow # Note updated model type and location path: tensorflow/mnist.savedmodel storage: s3: secretKey: wml-serving-example-models EOF predictor.wmlserving.ai.ibm.com/example-mnist-predictor configured $ kubectl get predictors NAME TYPE AVAILABLE ACTIVEMODEL TARGETMODEL TRANSITION AGE example-mnist-predictor sklearn true Loaded Loading InProgress 10m The \"transition\" state of the Predictor will be InProgress while waiting for the new backing model to be ready, and return to UpToDate once the transition is complete. $ kubectl get predictors NAME TYPE AVAILABLE ACTIVEMODEL TARGETMODEL TRANSITION AGE example-mnist-predictor sklearn true Loaded UpToDate 11m If there is a problem loading the new model (for example it does not exist at the specified path), the transition state will change to BlockedByFailedLoad , but the service will remain available. The active model state will still show as Loaded , and the Predictor remains available. $ kubectl get predictors NAME TYPE AVAILABLE ACTIVEMODEL TARGETMODEL TRANSITION AGE example-mnist-predictor sklearn true Loaded Failed BlockedByFailedLoad 20m For More Details Setup Storage Inferencing Predictor Spec A Jupyter Notebook of the example can also be found in wml-serving repo.","title":"Summary"},{"location":"predictors/#deploying-a-scikit-learn-model","text":"","title":"Deploying a scikit-learn model"},{"location":"predictors/#prerequisites","text":"The WML Serving instance should be installed in the desired namespace. See install docs for more details.","title":"Prerequisites"},{"location":"predictors/#deploy-a-sample-model-directly-from-our-shared-object-storage","text":"Check the storage-config secret for access to shared COS instance A set of example models are shared via an IBM Cloud COS instance to use when getting started with WML Serving and experimenting with the provided runtimes. Access to this COS instance is set up in the storage-config secret. If you used quick start install then there will be a key within the storage-config secret already configured with the name wml-serving-example-models . If you installed WML Serving using the operator, you will have to configure the storage-config secret for access: $ kubectl patch secret/storage-config -p '{\"data\": {\"wml-serving-example-models\": \"ewogICJ0eXBlIjogInMzIiwKICAiYWNjZXNzX2tleV9pZCI6ICJlY2I5ODNmMTE4MjI0MjNjYTllNDg3Zjg5OGQ1NGE4ZiIsCiAgInNlY3JldF9hY2Nlc3Nfa2V5IjogImNkYmVmZjZhMzJhZWY2YzIzNzRhZTY5ZWVmNTAzZTZkZDBjOTNkNmE3NGJjMjQ2NyIsCiAgImVuZHBvaW50X3VybCI6ICJodHRwczovL3MzLnVzLXNvdXRoLmNsb3VkLW9iamVjdC1zdG9yYWdlLmFwcGRvbWFpbi5jbG91ZCIsCiAgInJlZ2lvbiI6ICJ1cy1zb3V0aCIsCiAgImRlZmF1bHRfYnVja2V0IjogIndtbC1zZXJ2aW5nLWV4YW1wbGUtbW9kZWxzLXB1YmxpYyIKfQo=\"}}' For reference the contents of the secret value for the wml-serving-example-models entry looks like: { \"type\": \"s3\", \"access_key_id\": \"ecb983f11822423ca9e487f898d54a8f\", \"secret_access_key\": \"cdbeff6a32aef6c2374ae69eef503e6dd0c93d6a74bc2467\", \"endpoint_url\": \"https://s3.us-south.cloud-object-storage.appdomain.cloud\", \"region\": \"us-south\", \"default_bucket\": \"wml-serving-example-models-public\" } Note After updating the storage config secret, there may be a delay of up to 2 minutes until the change is picked up. You should take this into account when creating/updating Predictors that use storage keys which have just been added or updated - they may fail to load otherwise. For more details of configuring model storage, see the Setup Storage page. Create a Predictor Custom Resource to serve the sample model The config/example-predictors directory contains Predictor manifests for many of the example models. For a list of available models, see the example models documentation . Here we are deploying an sklearn model located at sklearn/mnist-svm.joblib within the shared COS storage. # Pulled from sample config/example-predictors/example-mlserver-sklearn-mnist-predictor.yaml $ kubectl apply -f - <<EOF apiVersion: wmlserving.ai.ibm.com/v1 kind: Predictor metadata: name: example-mnist-predictor spec: modelType: name: sklearn path: sklearn/mnist-svm.joblib storage: s3: secretKey: wml-serving-example-models EOF predictor.wmlserving.ai.ibm.com/example-mnist-predictor created Note that wml-serving-example-models is the name of the secret key created/verified in the previous step. For more details go to the Predictor Spec page . Once the Predictor is created, mlserver runtime pods are automatically started to load and serve it. $ kubectl get pods NAME READY STATUS RESTARTS AGE wml-serving-mlserver-0.x-658b7dd689-46nwm 0/3 ContainerCreating 0 2s wml-serving-mlserver-0.x-658b7dd689-46nwm 0/3 ContainerCreating 0 2s wmlserving-controller-568c45b959-nl88c 1/1 Running 0 11m Check the status of your Predictor: $ kubectl get predictors NAME TYPE AVAILABLE ACTIVEMODEL TARGETMODEL TRANSITION AGE example-mnist-predictor sklearn true Loading UpToDate 60s $ kubectl get predictor example-mnist-predictor -o=jsonpath='{.status.grpcEndpoint}' grpc://wml-serving:8033 The states should reflect immediate availability, but may take some seconds to move from Loading to Loaded . Inferencing requests for this Predictor received prior to loading completion will block until it completes. Note: When ScaleToZero is enabled, the first Predictor assigned to the Triton runtime may be stuck in the Pending state for some time while the Triton pods are being created. The Triton image is large and may take a while to download.","title":"Deploy a sample model directly from our shared object storage"},{"location":"predictors/#using-the-deployed-model","text":"Configure your gRPC client to point to address wml-serving:8033 . Use the protobuf-based gRPC inference service defined here to make inference requests to the model using the ModelInfer RPC, setting the name of the Predictor as the model_name field in the ModelInferRequest message. Here is an example of how to do this using the command-line based grpcurl : Port-forward to access the runtime service: # access via localhost:8033 $ kubectl port-forward service/wml-serving 8033 Forwarding from 127.0.0.1:8033 -> 8033 Forwarding from [::1]:8033 -> 8033 In a separate terminal window, send an inference request using the proto file from fvt/proto or one that you have locally. Note that you have to provide the model_name in the data load, which is the name of the Predictor deployed. $ grpcurl -plaintext -proto fvt/proto/kfs_inference_v2.proto localhost:8033 list inference.GRPCInferenceService # run inference # with below input, expect output to be 8 $ grpcurl -plaintext -proto fvt/proto/kfs_inference_v2.proto -d '{ \"model_name\": \"example-mnist-predictor\", \"inputs\": [{ \"name\": \"predict\", \"shape\": [1, 64], \"datatype\": \"FP32\", \"contents\": { \"fp32_contents\": [0.0, 0.0, 1.0, 11.0, 14.0, 15.0, 3.0, 0.0, 0.0, 1.0, 13.0, 16.0, 12.0, 16.0, 8.0, 0.0, 0.0, 8.0, 16.0, 4.0, 6.0, 16.0, 5.0, 0.0, 0.0, 5.0, 15.0, 11.0, 13.0, 14.0, 0.0, 0.0, 0.0, 0.0, 2.0, 12.0, 16.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 13.0, 16.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 16.0, 16.0, 16.0, 7.0, 0.0, 0.0, 0.0, 0.0, 11.0, 13.0, 12.0, 1.0, 0.0] }}]}' localhost:8033 inference.GRPCInferenceService.ModelInfer { \"modelName\": \"example-mnist-predictor-725d74f061\", \"outputs\": [ { \"name\": \"predict\", \"datatype\": \"FP32\", \"shape\": [ \"1\" ], \"contents\": { \"fp32Contents\": [ 8 ] } } ] }","title":"Using the deployed model"},{"location":"predictors/#updating-the-model","text":"Changes can be made to the Predictor's Spec, such as changing the target storage and/or model, without interrupting the inferencing service. The predictor will continue to use the prior spec/model until the new one is loaded and ready. Below, we are changing the Predictor to use a completely different model, in practice the schema of the Predictor's model would be consistent across updates even if the type of model or ML framework changes. $ kubectl apply -f - <<EOF apiVersion: wmlserving.ai.ibm.com/v1 kind: Predictor metadata: name: example-mnist-predictor spec: modelType: name: tensorflow # Note updated model type and location path: tensorflow/mnist.savedmodel storage: s3: secretKey: wml-serving-example-models EOF predictor.wmlserving.ai.ibm.com/example-mnist-predictor configured $ kubectl get predictors NAME TYPE AVAILABLE ACTIVEMODEL TARGETMODEL TRANSITION AGE example-mnist-predictor sklearn true Loaded Loading InProgress 10m The \"transition\" state of the Predictor will be InProgress while waiting for the new backing model to be ready, and return to UpToDate once the transition is complete. $ kubectl get predictors NAME TYPE AVAILABLE ACTIVEMODEL TARGETMODEL TRANSITION AGE example-mnist-predictor sklearn true Loaded UpToDate 11m If there is a problem loading the new model (for example it does not exist at the specified path), the transition state will change to BlockedByFailedLoad , but the service will remain available. The active model state will still show as Loaded , and the Predictor remains available. $ kubectl get predictors NAME TYPE AVAILABLE ACTIVEMODEL TARGETMODEL TRANSITION AGE example-mnist-predictor sklearn true Loaded Failed BlockedByFailedLoad 20m","title":"Updating the model"},{"location":"predictors/#for-more-details","text":"Setup Storage Inferencing Predictor Spec A Jupyter Notebook of the example can also be found in wml-serving repo.","title":"For More Details"},{"location":"predictors/predictor-spec/","text":"Predictor Spec Here is a complete example of a Predictor spec: apiVersion: wmlserving.ai.ibm.com/v1 kind: Predictor metadata: name: my-mnist-predictor spec: modelType: name: tensorflow version: \"1.15\" # Optional runtime: # Optional name: triton-2.x path: my_models/mnist-tf storage: s3: secretKey: my_storage bucket: my_bucket Notes: Prior to version 0.3.0 , please use apiVersion: ai.ibm.com/v1alpha1 in the Predictor yaml Prior to version 0.5.0 , please use apiVersion: wmlserving.ai.ibm.com/v1alpha1 in the Predictor yaml runtime is optional. If included, the model will be loaded/served using the ServingRuntime with the specified name, and the predictors modelType must match an entry in that runtime's supportedModels list (see runtimes ) The CRD contains additional fields but they have been omitted here for now since they are not yet fully supported","title":"Predictor Spec"},{"location":"predictors/predictor-spec/#predictor-spec","text":"Here is a complete example of a Predictor spec: apiVersion: wmlserving.ai.ibm.com/v1 kind: Predictor metadata: name: my-mnist-predictor spec: modelType: name: tensorflow version: \"1.15\" # Optional runtime: # Optional name: triton-2.x path: my_models/mnist-tf storage: s3: secretKey: my_storage bucket: my_bucket Notes: Prior to version 0.3.0 , please use apiVersion: ai.ibm.com/v1alpha1 in the Predictor yaml Prior to version 0.5.0 , please use apiVersion: wmlserving.ai.ibm.com/v1alpha1 in the Predictor yaml runtime is optional. If included, the model will be loaded/served using the ServingRuntime with the specified name, and the predictors modelType must match an entry in that runtime's supportedModels list (see runtimes ) The CRD contains additional fields but they have been omitted here for now since they are not yet fully supported","title":"Predictor Spec"},{"location":"predictors/run-inference/","text":"Send an inference request to your Predictor Configure gRPC client Configure your gRPC client to point to address wml-serving:8033 , which is based on the kube-dns address and port corresponding to the service. Use the protobuf-based gRPC inference service defined here to make inference requests to the model using the ModelInfer RPC, setting the name of the Predictor as the model_name field in the ModelInferRequest message. Configure the gRPC clients which talk to your service to explicitly use: The round_robin loadbalancer policy A target URI string starting with dns:// and based on the kube-dns address and port corresponding to the service, for example dns:///model-mesh-test.wml-serving:8033 where wml-serving is the namespace, or just dns:///model-mesh-test:8033 if the client resides in the same namespace. Note that you end up needing three consecutive / 's in total. Not all languages have built-in support for this but most of the primary ones do. It's recommended to use the latest version of gRPC regardless. Here are some examples for specific languages (note other config such as TLS is omitted): Java ManagedChannel channel = NettyChannelBuilder.forTarget(\"wml-serving:8033\") .defaultLoadBalancingPolicy(\"round_robin\").build(); Note that this was done differently in earlier versions of grpc-java - if this does not compile ensure you upgrade. Go ctx, cancel := context.WithTimeout(context.Background(), 5 * time.Second) defer cancel() grpc.DialContext(ctx, \"wml-serving:8033\", grpc.WithBlock(), grpc.WithDefaultServiceConfig(`{\"loadBalancingPolicy\":\"round_robin\"}`)) Python credentials = grpc.ssl_channel_credentials(certificate_data_bytes) channel_options = ((\"grpc.lb_policy_name\", \"round_robin\"),) channel = grpc.secure_channel(target, credentials, options=channel_options) NodeJS Using: https://www.npmjs.com/package/grpc // Read certificate const cert = readFileSync(sslCertPath); credentials = grpc.credentials.createSsl(cert); // For insecure credentials = grpc.credentials.createInsecure(); // Create client const clientOptions = { \"grpc.lb_policy_name\": \"round_robin\", }; // Get ModelMeshClient from grpc protobuf file const client = ModelMeshClient(model_mesh_uri, credentials, clientOptions); // Get rpc prototype for server const response = await rpcProtoType.call(client, message); How to access service from outside the cluster without a NodePort Using kubectl port-forward : kubectl port-forward wml-serving 8033:8033 This assumes you are using port 8033, change the source and/or destination ports as appropriate. Then change your client target string to localhost:8033, where 8033 is the chosen source port. grpcurl Example Here is an example of how to do this using the command-line based grpcurl : Install grpcurl: $ grpcurl --version grpcurl 1.8.1 # If it doesn't exist $ brew install grpcurl Port-forward to access the runtime service: # access via localhost:8033 $ kubectl port-forward service/wml-serving 8033 Forwarding from 127.0.0.1:8033 -> 8033 Forwarding from [::1]:8033 -> 8033 In a separate terminal window, send an inference request using the proto file from fvt/proto or one that you have locally: $ grpcurl -plaintext -proto fvt/proto/kfs_inference_v2.proto localhost:8033 list inference.GRPCInferenceService # run inference # with below input, expect output to be 8 $ grpcurl \\ -plaintext \\ -proto fvt/proto/kfs_inference_v2.proto \\ -d '{ \"model_name\": \"example-mnist-predictor\", \"inputs\": [{ \"name\": \"predict\", \"shape\": [1, 64], \"datatype\": \"FP32\", \"contents\": { \"fp32_contents\": [0.0, 0.0, 1.0, 11.0, 14.0, 15.0, 3.0, 0.0, 0.0, 1.0, 13.0, 16.0, 12.0, 16.0, 8.0, 0.0, 0.0, 8.0, 16.0, 4.0, 6.0, 16.0, 5.0, 0.0, 0.0, 5.0, 15.0, 11.0, 13.0, 14.0, 0.0, 0.0, 0.0, 0.0, 2.0, 12.0, 16.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 13.0, 16.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 16.0, 16.0, 16.0, 7.0, 0.0, 0.0, 0.0, 0.0, 11.0, 13.0, 12.0, 1.0, 0.0] }}]}' \\ localhost:8033 \\ inference.GRPCInferenceService.ModelInfer { \"modelName\": \"example-mnist-predictor-725d74f061\", \"outputs\": [ { \"name\": \"predict\", \"datatype\": \"FP32\", \"shape\": [ \"1\" ], \"contents\": { \"fp32Contents\": [ 8 ] } } ] } Note that you have to provide the model_name in the data load, which is the name of the Predictor deployed. If a custom serving runtime which doesn't use the KFS V2 API is being used, the mm-vmodel-id header must be set to the Predictor name. If you are sure the requests from your client are being routed in such a way that balances evenly across the cluster (as described above ), you should include an additional metadata parameter mm-balanced = true . This allows some internal performance optimizations but should not be included if the source if the requests is not properly balanced. For example adding these headers to the above grpcurl command: grpcurl \\ -plaintext \\ -proto fvt/proto/kfs_inference_v2.proto \\ -rpc-header mm-vmodel-id:example-sklearn-mnist-svm \\ -rpc-header mm-balanced:true \\ -d '{ \"model_name\": \"example-sklearn-mnist-svm\", \"inputs\": [{ \"name\": \"predict\", \"shape\": [1, 64], \"datatype\": \"FP32\", \"contents\": { \"fp32_contents\": [0.0, 0.0, 1.0, 11.0, 14.0, 15.0, 3.0, 0.0, 0.0, 1.0, 13.0, 16.0, 12.0, 16.0, 8.0, 0.0, 0.0, 8.0, 16.0, 4.0, 6.0, 16.0, 5.0, 0.0, 0.0, 5.0, 15.0, 11.0, 13.0, 14.0, 0.0, 0.0, 0.0, 0.0, 2.0, 12.0, 16.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 13.0, 16.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 16.0, 16.0, 16.0, 7.0, 0.0, 0.0, 0.0, 0.0, 11.0, 13.0, 12.0, 1.0, 0.0] }}]}' \\ localhost:8033 \\ inference.GRPCInferenceService.ModelInfer A Jupyter Notebook of the example with an additional Tensorflow example can also be found in wml-serving repo.","title":"Inferencing"},{"location":"predictors/run-inference/#send-an-inference-request-to-your-predictor","text":"","title":"Send an inference request to your Predictor"},{"location":"predictors/run-inference/#configure-grpc-client","text":"Configure your gRPC client to point to address wml-serving:8033 , which is based on the kube-dns address and port corresponding to the service. Use the protobuf-based gRPC inference service defined here to make inference requests to the model using the ModelInfer RPC, setting the name of the Predictor as the model_name field in the ModelInferRequest message. Configure the gRPC clients which talk to your service to explicitly use: The round_robin loadbalancer policy A target URI string starting with dns:// and based on the kube-dns address and port corresponding to the service, for example dns:///model-mesh-test.wml-serving:8033 where wml-serving is the namespace, or just dns:///model-mesh-test:8033 if the client resides in the same namespace. Note that you end up needing three consecutive / 's in total. Not all languages have built-in support for this but most of the primary ones do. It's recommended to use the latest version of gRPC regardless. Here are some examples for specific languages (note other config such as TLS is omitted):","title":"Configure gRPC client"},{"location":"predictors/run-inference/#java","text":"ManagedChannel channel = NettyChannelBuilder.forTarget(\"wml-serving:8033\") .defaultLoadBalancingPolicy(\"round_robin\").build(); Note that this was done differently in earlier versions of grpc-java - if this does not compile ensure you upgrade.","title":"Java"},{"location":"predictors/run-inference/#go","text":"ctx, cancel := context.WithTimeout(context.Background(), 5 * time.Second) defer cancel() grpc.DialContext(ctx, \"wml-serving:8033\", grpc.WithBlock(), grpc.WithDefaultServiceConfig(`{\"loadBalancingPolicy\":\"round_robin\"}`))","title":"Go"},{"location":"predictors/run-inference/#python","text":"credentials = grpc.ssl_channel_credentials(certificate_data_bytes) channel_options = ((\"grpc.lb_policy_name\", \"round_robin\"),) channel = grpc.secure_channel(target, credentials, options=channel_options)","title":"Python"},{"location":"predictors/run-inference/#nodejs","text":"Using: https://www.npmjs.com/package/grpc // Read certificate const cert = readFileSync(sslCertPath); credentials = grpc.credentials.createSsl(cert); // For insecure credentials = grpc.credentials.createInsecure(); // Create client const clientOptions = { \"grpc.lb_policy_name\": \"round_robin\", }; // Get ModelMeshClient from grpc protobuf file const client = ModelMeshClient(model_mesh_uri, credentials, clientOptions); // Get rpc prototype for server const response = await rpcProtoType.call(client, message);","title":"NodeJS"},{"location":"predictors/run-inference/#how-to-access-service-from-outside-the-cluster-without-a-nodeport","text":"Using kubectl port-forward : kubectl port-forward wml-serving 8033:8033 This assumes you are using port 8033, change the source and/or destination ports as appropriate. Then change your client target string to localhost:8033, where 8033 is the chosen source port.","title":"How to access service from outside the cluster without a NodePort"},{"location":"predictors/run-inference/#grpcurl-example","text":"Here is an example of how to do this using the command-line based grpcurl : Install grpcurl: $ grpcurl --version grpcurl 1.8.1 # If it doesn't exist $ brew install grpcurl Port-forward to access the runtime service: # access via localhost:8033 $ kubectl port-forward service/wml-serving 8033 Forwarding from 127.0.0.1:8033 -> 8033 Forwarding from [::1]:8033 -> 8033 In a separate terminal window, send an inference request using the proto file from fvt/proto or one that you have locally: $ grpcurl -plaintext -proto fvt/proto/kfs_inference_v2.proto localhost:8033 list inference.GRPCInferenceService # run inference # with below input, expect output to be 8 $ grpcurl \\ -plaintext \\ -proto fvt/proto/kfs_inference_v2.proto \\ -d '{ \"model_name\": \"example-mnist-predictor\", \"inputs\": [{ \"name\": \"predict\", \"shape\": [1, 64], \"datatype\": \"FP32\", \"contents\": { \"fp32_contents\": [0.0, 0.0, 1.0, 11.0, 14.0, 15.0, 3.0, 0.0, 0.0, 1.0, 13.0, 16.0, 12.0, 16.0, 8.0, 0.0, 0.0, 8.0, 16.0, 4.0, 6.0, 16.0, 5.0, 0.0, 0.0, 5.0, 15.0, 11.0, 13.0, 14.0, 0.0, 0.0, 0.0, 0.0, 2.0, 12.0, 16.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 13.0, 16.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 16.0, 16.0, 16.0, 7.0, 0.0, 0.0, 0.0, 0.0, 11.0, 13.0, 12.0, 1.0, 0.0] }}]}' \\ localhost:8033 \\ inference.GRPCInferenceService.ModelInfer { \"modelName\": \"example-mnist-predictor-725d74f061\", \"outputs\": [ { \"name\": \"predict\", \"datatype\": \"FP32\", \"shape\": [ \"1\" ], \"contents\": { \"fp32Contents\": [ 8 ] } } ] } Note that you have to provide the model_name in the data load, which is the name of the Predictor deployed. If a custom serving runtime which doesn't use the KFS V2 API is being used, the mm-vmodel-id header must be set to the Predictor name. If you are sure the requests from your client are being routed in such a way that balances evenly across the cluster (as described above ), you should include an additional metadata parameter mm-balanced = true . This allows some internal performance optimizations but should not be included if the source if the requests is not properly balanced. For example adding these headers to the above grpcurl command: grpcurl \\ -plaintext \\ -proto fvt/proto/kfs_inference_v2.proto \\ -rpc-header mm-vmodel-id:example-sklearn-mnist-svm \\ -rpc-header mm-balanced:true \\ -d '{ \"model_name\": \"example-sklearn-mnist-svm\", \"inputs\": [{ \"name\": \"predict\", \"shape\": [1, 64], \"datatype\": \"FP32\", \"contents\": { \"fp32_contents\": [0.0, 0.0, 1.0, 11.0, 14.0, 15.0, 3.0, 0.0, 0.0, 1.0, 13.0, 16.0, 12.0, 16.0, 8.0, 0.0, 0.0, 8.0, 16.0, 4.0, 6.0, 16.0, 5.0, 0.0, 0.0, 5.0, 15.0, 11.0, 13.0, 14.0, 0.0, 0.0, 0.0, 0.0, 2.0, 12.0, 16.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 13.0, 16.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 16.0, 16.0, 16.0, 7.0, 0.0, 0.0, 0.0, 0.0, 11.0, 13.0, 12.0, 1.0, 0.0] }}]}' \\ localhost:8033 \\ inference.GRPCInferenceService.ModelInfer A Jupyter Notebook of the example with an additional Tensorflow example can also be found in wml-serving repo.","title":"grpcurl Example"},{"location":"predictors/setup-storage/","text":"You will need access to an S3-compatible object storage, for example MinIO or IBM Cloud Object Storage . To provide access to the object storage, use the storage-config secret. Deploy a sample model from our shared object storage A set of example models are shared via an IBM Cloud COS instance to use when getting started with WML Serving and experimenting with the provided runtimes. Access to this COS instance needs to be set up in the storage-config secret. For the shared COS instance, the secretKey to access the models is wml-serving-example-models . If you used the --quickstart option of the install script, the secretKey to use for the local MinIO storage instance is localMinIO . If you use a different key value, be sure to update the spec.storage.s3.secretKey value in the Predictor. It should look something like: $ kubectl describe secret storage-config Name: storage-config Namespace: wml-serving Labels: indigo-service=all Annotations: Type: Opaque Data ==== wml-serving-example-models: 308 bytes If you installed WML Serving using the operator, you will have to configure the storage-config secret for access: $ kubectl patch secret/storage-config -p '{\"data\": {\"wml-serving-example-models\": \"ewogICJ0eXBlIjogInMzIiwKICAiYWNjZXNzX2tleV9pZCI6ICJlY2I5ODNmMTE4MjI0MjNjYTllNDg3Zjg5OGQ1NGE4ZiIsCiAgInNlY3JldF9hY2Nlc3Nfa2V5IjogImNkYmVmZjZhMzJhZWY2YzIzNzRhZTY5ZWVmNTAzZTZkZDBjOTNkNmE3NGJjMjQ2NyIsCiAgImVuZHBvaW50X3VybCI6ICJodHRwczovL3MzLnVzLXNvdXRoLmNsb3VkLW9iamVjdC1zdG9yYWdlLmFwcGRvbWFpbi5jbG91ZCIsCiAgInJlZ2lvbiI6ICJ1cy1zb3V0aCIsCiAgImRlZmF1bHRfYnVja2V0IjogIndtbC1zZXJ2aW5nLWV4YW1wbGUtbW9kZWxzLXB1YmxpYyIKfQo=\"}}' For reference the contents of the secret value for the wml-serving-example-models entry looks like: { \"type\": \"s3\", \"access_key_id\": \"ecb983f11822423ca9e487f898d54a8f\", \"secret_access_key\": \"cdbeff6a32aef6c2374ae69eef503e6dd0c93d6a74bc2467\", \"endpoint_url\": \"https://s3.us-south.cloud-object-storage.appdomain.cloud\", \"region\": \"us-south\", \"default_bucket\": \"wml-serving-example-models-public\" } Note After updating the storage config secret, there may be a delay of up to 2 minutes until the change is picked up. You should take this into account when creating/updating Predictors that use storage keys which have just been added or updated - they may fail to load otherwise. Deploy a model from your own object storage Download sample model or use an existing model Here we show an example using an MLeap-format model for AirBnB Linear Regression . Add your MLeap saved model to S3-based object storage A bucket in MinIO needs to be created to copy the model into, which either requires MinIO Client or port-forwarding the minio service and logging in using the web interface. # Install minio client $ brew install minio/stable/mc $ mc --help NAME: mc - MinIO Client for cloud storage and filesystems. .... # test setup - mc is pre-configured with https://play.min.io, aliased as \"play\". # list all buckets in play $ mc ls play [2021-06-10 21:04:25 EDT] 0B 2063b651-92a3-4a20-a4a5-03a96e7c5a89/ [2021-06-11 02:40:33 EDT] 0B 5ddfe44282319c500c3a4f9b/ [2021-06-11 05:15:45 EDT] 0B 6dkmmiqcdho1zoloomsj3620cocs6iij/ [2021-06-11 02:39:54 EDT] 0B 9jo5omejcyyr62iizn02ex982eapipjr/ [2021-06-11 02:33:53 EDT] 0B a-test-zip/ [2021-06-11 09:14:28 EDT] 0B aio-ato/ [2021-06-11 09:14:29 EDT] 0B aio-ato-art/ ... # add cloud storage service $ mc alias set <ALIAS> <YOUR-S3-ENDPOINT> [YOUR-ACCESS-KEY] [YOUR-SECRET-KEY] # for example if you installed with --quickstart $ mc alias set myminio http://localhost:9000 EXAMPLE_ACESS_KEY example/secret/EXAMPLEKEY Added `myminio` successfully. # create bucket $ mc mb myminio/models/mleap Bucket created successfully myminio/models/mleap. $ mc tree myminio myminio \u2514\u2500 models \u2514\u2500 mleap # copy object -- must copy into an existing bucket $ mc cp ~/Downloads/airbnb.model.lr.zip myminio/models/mleap ...model.lr.zip: 14.90 KiB / 14.90 KiB \u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593 2.74 MiB/s 0s $ mc ls myminio/models/mleap [2021-06-11 11:55:48 EDT] 15KiB airbnb.model.lr.zip Add a storage entry to the storage-config secret Ensure there is a key defined in the common storage-config secret corresponding to the S3-based storage instance holding your model. The value of this secret key should be JSON like the following, default_bucket is optional. Users can specify use of a custom certificate via the storage config certificate parameter. The custom certificate should be in the form of an embedded Certificate Authority (CA) bundle in PEM format. Using MinIO the JSON contents look like: { \"type\": \"s3\", \"access_key_id\": \"minioadmin\", \"secret_access_key\": \"minioadmin/K7JTCMP/EXAMPLEKEY\", \"endpoint_url\": \"http://127.0.0.1:9000:9000\", \"default_bucket\": \"\", \"region\": \"us-east\" } Remember that after updating the storage config secret, there may be a delay of up to 2 minutes until the change is picked up. You should take this into account when creating/updating Predictors that use storage keys which have just been added or updated - they may fail to load otherwise.","title":"Set up Storage for Loading Models"},{"location":"predictors/setup-storage/#deploy-a-sample-model-from-our-shared-object-storage","text":"A set of example models are shared via an IBM Cloud COS instance to use when getting started with WML Serving and experimenting with the provided runtimes. Access to this COS instance needs to be set up in the storage-config secret. For the shared COS instance, the secretKey to access the models is wml-serving-example-models . If you used the --quickstart option of the install script, the secretKey to use for the local MinIO storage instance is localMinIO . If you use a different key value, be sure to update the spec.storage.s3.secretKey value in the Predictor. It should look something like: $ kubectl describe secret storage-config Name: storage-config Namespace: wml-serving Labels: indigo-service=all Annotations: Type: Opaque Data ==== wml-serving-example-models: 308 bytes If you installed WML Serving using the operator, you will have to configure the storage-config secret for access: $ kubectl patch secret/storage-config -p '{\"data\": {\"wml-serving-example-models\": \"ewogICJ0eXBlIjogInMzIiwKICAiYWNjZXNzX2tleV9pZCI6ICJlY2I5ODNmMTE4MjI0MjNjYTllNDg3Zjg5OGQ1NGE4ZiIsCiAgInNlY3JldF9hY2Nlc3Nfa2V5IjogImNkYmVmZjZhMzJhZWY2YzIzNzRhZTY5ZWVmNTAzZTZkZDBjOTNkNmE3NGJjMjQ2NyIsCiAgImVuZHBvaW50X3VybCI6ICJodHRwczovL3MzLnVzLXNvdXRoLmNsb3VkLW9iamVjdC1zdG9yYWdlLmFwcGRvbWFpbi5jbG91ZCIsCiAgInJlZ2lvbiI6ICJ1cy1zb3V0aCIsCiAgImRlZmF1bHRfYnVja2V0IjogIndtbC1zZXJ2aW5nLWV4YW1wbGUtbW9kZWxzLXB1YmxpYyIKfQo=\"}}' For reference the contents of the secret value for the wml-serving-example-models entry looks like: { \"type\": \"s3\", \"access_key_id\": \"ecb983f11822423ca9e487f898d54a8f\", \"secret_access_key\": \"cdbeff6a32aef6c2374ae69eef503e6dd0c93d6a74bc2467\", \"endpoint_url\": \"https://s3.us-south.cloud-object-storage.appdomain.cloud\", \"region\": \"us-south\", \"default_bucket\": \"wml-serving-example-models-public\" } Note After updating the storage config secret, there may be a delay of up to 2 minutes until the change is picked up. You should take this into account when creating/updating Predictors that use storage keys which have just been added or updated - they may fail to load otherwise.","title":"Deploy a sample model from our shared object storage"},{"location":"predictors/setup-storage/#deploy-a-model-from-your-own-object-storage","text":"Download sample model or use an existing model Here we show an example using an MLeap-format model for AirBnB Linear Regression . Add your MLeap saved model to S3-based object storage A bucket in MinIO needs to be created to copy the model into, which either requires MinIO Client or port-forwarding the minio service and logging in using the web interface. # Install minio client $ brew install minio/stable/mc $ mc --help NAME: mc - MinIO Client for cloud storage and filesystems. .... # test setup - mc is pre-configured with https://play.min.io, aliased as \"play\". # list all buckets in play $ mc ls play [2021-06-10 21:04:25 EDT] 0B 2063b651-92a3-4a20-a4a5-03a96e7c5a89/ [2021-06-11 02:40:33 EDT] 0B 5ddfe44282319c500c3a4f9b/ [2021-06-11 05:15:45 EDT] 0B 6dkmmiqcdho1zoloomsj3620cocs6iij/ [2021-06-11 02:39:54 EDT] 0B 9jo5omejcyyr62iizn02ex982eapipjr/ [2021-06-11 02:33:53 EDT] 0B a-test-zip/ [2021-06-11 09:14:28 EDT] 0B aio-ato/ [2021-06-11 09:14:29 EDT] 0B aio-ato-art/ ... # add cloud storage service $ mc alias set <ALIAS> <YOUR-S3-ENDPOINT> [YOUR-ACCESS-KEY] [YOUR-SECRET-KEY] # for example if you installed with --quickstart $ mc alias set myminio http://localhost:9000 EXAMPLE_ACESS_KEY example/secret/EXAMPLEKEY Added `myminio` successfully. # create bucket $ mc mb myminio/models/mleap Bucket created successfully myminio/models/mleap. $ mc tree myminio myminio \u2514\u2500 models \u2514\u2500 mleap # copy object -- must copy into an existing bucket $ mc cp ~/Downloads/airbnb.model.lr.zip myminio/models/mleap ...model.lr.zip: 14.90 KiB / 14.90 KiB \u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593 2.74 MiB/s 0s $ mc ls myminio/models/mleap [2021-06-11 11:55:48 EDT] 15KiB airbnb.model.lr.zip Add a storage entry to the storage-config secret Ensure there is a key defined in the common storage-config secret corresponding to the S3-based storage instance holding your model. The value of this secret key should be JSON like the following, default_bucket is optional. Users can specify use of a custom certificate via the storage config certificate parameter. The custom certificate should be in the form of an embedded Certificate Authority (CA) bundle in PEM format. Using MinIO the JSON contents look like: { \"type\": \"s3\", \"access_key_id\": \"minioadmin\", \"secret_access_key\": \"minioadmin/K7JTCMP/EXAMPLEKEY\", \"endpoint_url\": \"http://127.0.0.1:9000:9000\", \"default_bucket\": \"\", \"region\": \"us-east\" } Remember that after updating the storage config secret, there may be a delay of up to 2 minutes until the change is picked up. You should take this into account when creating/updating Predictors that use storage keys which have just been added or updated - they may fail to load otherwise.","title":"Deploy a model from your own object storage"}]}